# -*- coding: utf-8 -*-
"""
Created on Sun Jun 28 2023

@author: lakshya

#The prototype successfuly finds out vulnerable services on a network
#This prototype scans 10,000 ports for 50 episodes
#Hyperparameters are modified
#Changes are made in the reward system, penalty is much lower as the number of states is increased
#Data such as reward system, Q-Values, Time and etx is gathered to gather and analyse the performance of the prototype


"""

import os
import time
import pandas as pd
import nmap
import numpy as np
import json
from fuzzywuzzy import fuzz

class NetworkEnv:
    def __init__(self, target_ip, target_ports, vulnerability_database_path):
        
        self.target_ip = target_ip
        self.target_ports = target_ports
        with open(vulnerability_database_path, 'r', encoding='utf-8') as file:
            self.vulnerability_database = json.load(file)
        self.current_port = 0
        self.nm = nmap.PortScanner()
        
    def load_vulnerability_database(self, filepath):
        with open(filepath, 'r') as file:
            data = json.load(file)
        return data
    
    def step(self, action):
            info = {}
            reward = 0
            done = False
            
            
            #If the action is to scan
            if action == 0:
                print(f"Scanning port: {self.target_ports[self.current_port]}")
                result = self.nm.scan(self.target_ip, str(self.target_ports[self.current_port]), arguments='-sV')
                

                #Check for the service and the version
                service = result['scan'][self.target_ip]['tcp'][self.target_ports[self.current_port]]['name']
                version = result['scan'][self.target_ip]['tcp'][self.target_ports[self.current_port]]['version']
                
                #Check if they are in the vulnerability database
                for vuln in self.vulnerability_database:
                    service_similarity = fuzz.ratio(vuln['Service'].lower(), service.lower())
                    if service_similarity >= 80 and vuln['Version'] == version:
                        reward = 100
                        info = {"Vulnerability": vuln['CVE ID'], "Port": self.target_ports[self.current_port]}
                        print(f"Vulnerability found at episode: {episodes} - CVE ID: {info['Vulnerability']} on Port: {info['Port']}")
                        break
                    else:
                        reward = -0.05
            else:
                #Action set to Do Not Scan
                reward = 0 #No penalty for skipping non-vulnerable port
            
            
            if self.current_port >= len(self.target_ports) - 1:
                done = True
            else:
                self.current_port += 1
            return self.current_port, reward, done, info
        
    def reset(self):
            self.current_port = 0
            return self.current_port
        
class QLAgent:
    #Changed alpha to 0.3 from 0.5 -- This defines by how much new information overrides old information
    #Tried discount factor (epsilon_decay) to 0.9 but that messe up the training
    def __init__(self, num_states, num_actions, alpha=0.3 , gamma=0.95, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        self.num_actions = num_actions
        
        #Initialising Q-table to small random values
        self.q_table = np.random.uniform(low=-1, high=1, size=(num_states, num_actions))

        #List to store file names
        self.q_table_filenames = []
        
    def get_action(self, state):
        if np.random.rand() <= self.epsilon:
            return np.random.choice(self.num_actions) #Exploration
        
        
        else:
            return np.argmax(self.q_table[state]) #Exploitation
    
    def update_q_values(self, state, action, reward, next_state):
        old_value = self.q_table[state, action]
        next_max = np.max(self.q_table[next_state])
        
        new_value = (1 - self.alpha) * old_value + self.alpha * (reward + self.gamma * next_max)
        self.q_table[state, action] = new_value
        
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

    def save_q_table(self, episode_num):
        filename = f"q_table_{episode_num}.npy"
        np.save(filename, self.q_table)

        #Append the filename to the list
        self.q_table_filenames.append(filename)

        #Delete the olde ones if the list is more than 10 files
        if len(self.q_table_filenames) > 10:
            oldest_file = self.q_table_filenames.pop(0)
            os.remove(oldest_file)


            
#Environment Initialization
target_ip = '192.168.240.129'
target_ports = list(range(1,10000)) #Port scan from 1 to 100, gotta make it for the entire range, both for TCP and UDP
vulnerability_database_path = "vulnerability_database_2023.json" #Database containing known vulnerabilities
env = NetworkEnv(target_ip, target_ports, vulnerability_database_path)

num_states = len(target_ports)
num_actions = 2 #0=Scan and 1=Don't scan
agent = QLAgent(num_states, num_actions)

total_rewards = []
episode_lengths = []

#Variables to store data for analysing later
vulnerabilities_per_episode = []
epsiode_durations = []
all_episode_rewards = []
max_q_values_list = []


#Training the agent
num_episodes = 50
for episodes in range(num_episodes):

    #Episode Duration
    start_time = time.time()
    print(f"Starting Episode: {episodes}")
    state = env.reset()
    episode_rewards = []
    total_reward = 0  # Initialize total_reward here
    episode_length = 0  # Initialize episode_length here
    vulnerabilities_detected = 0
    
    for step in range(num_states):
        action = agent.get_action(state)
        next_state, reward, done, info = env.step(action)
        agent.update_q_values(state, action, reward, next_state)
        
        episode_rewards.append(reward)
        total_reward += reward
        episode_length += 1
        
        if 'Vulnerability' in info:
            vulnerabilities_detected += 1

        if done:
            #print(f"Vulnerability found at episode: {episodes} - CVE ID: {info['Vulnerability']} on Port: {info['Port']}")
            
            #Converting Q-Table to DataFrame for easier viewing
            #Saving the DataFrame to a csv file
            
            break
        
        state = next_state
    #For episode duration
    end_time = time.time()
    epsiode_durations.append(end_time - start_time)

    #For collecting reward distribution information
    all_episode_rewards.append(episode_rewards)

    #For Q-value distribution every 10 episodes
    if episodes % 5 == 0:
        max_q_values =  np.max(agent.q_table, axis=1)
        max_q_values_list.append(max_q_values)

    #Append the number of vulnerabilities detected in this episode to the list
    vulnerabilities_per_episode.append(vulnerabilities_detected)

    agent.save_q_table(episodes)
    
    total_rewards.append(total_reward)
    episode_lengths.append(episode_length)

print("Total rewards per episode: ", total_rewards)  # You need a comma instead of a period here
print("Episode lengths: ", episode_lengths)
print("Vulnerabilities detected per episode:", vulnerabilities_per_episode)
# Print episode durations
print("\nEpisode Durations:")
for i, duration in enumerate(episode_durations, 1):
    print(f"Episode {i}: {duration:.2f} seconds")

# Print total rewards for each episode
print("\nTotal Rewards per Episode:")
for i, rewards in enumerate(all_episode_rewards, 1):
    print(f"Episode {i}: Total reward = {sum(rewards)}")

# Print average reward for each episode
print("\nAverage Reward per Episode:")
for i, rewards in enumerate(all_episode_rewards, 1):
    print(f"Episode {i}: Average reward = {sum(rewards) / len(rewards):.2f}")

# Print max Q-values for episodes where they were saved
print("\nMax Q-Values Snapshot (every 10 episodes):")
for i, q_values in enumerate(max_q_values_list, 1):
    print(f"Episode {i*10}: Max Q-values = {q_values}")
